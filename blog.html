<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Nasya Choy</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css" />
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Ubuntu:regular,bold&subset=Latin">
  </head>

  <body class="earth">
    <!-- Navigation -->
    <nav class="navigation">
      <div class="navigationContainer">
        <ul class="navigationList">
          <li class="navigationObject">
            <a href="./blog.html" class="navigationLink"><img class="planet" id="Mars" src="./Images/Mars.png" /></a>
          </li>
          <li class="navigationObject">
            <a href="./index.html" class="navigationLink"><img class="planet" id="Earth" src="./Images/Earth.png" /></a>
          </li>
          <li class="navigationObject">
            <a href="./food.html" class="navigationLink"><img class="planet" id="Venus" src="./Images/Venus.png" /></a>
          </li>
          <li class="navigationObject">
            <a href="./projects.html" class="navigationLink"><img class="planet" id="Mercury" src="./Images/Mercury.png" /></a>
          </li>
        </ul>
      </div>
      <div id="navigationToggle">
        <img id="Sun" src="./Images/Sun.png" />
      </div>
    </nav>

    <!-- Blog Post 1 -->
    <div class="blogPost" id = "post1">
      <div class="postContainer">
        <h1>Week 1 Recap: 5/26–5/30</h1>

        <p>I arrived at Huntsville, Alabama on 5/26, but not even a week later I feel like my brain has grown threefold. In just five days, I moved from Massachusetts to Alabama, completed training, toured the facility, met dozens of new people, created a blog, researched persistent homology, learned the basics of GUDHI, and went to Walmart at least three times (like a true Arkansan).</p>

        <p>For this summer, I'll be working with the Leidos Dynetics AI and Machine Learning Branch on a project exploring persistent homology methods and its applications in aerospace. As such, our mentor Mundy ensured we met a majority of the people in the team through multiple lunches and tours (of people--not of the facilities). Everyone I met was happy to make conversation about their projects and provide support for anything. The vibes were immaculate and the culture of Leidos Dynetics as a whole was welcoming.</p>

        <p>While I believe I don't have the strongest coding background coming into this internship, I'm confident I'll leave with a deep understanding. This in-depth experience of working on a machine learning research project while supported by a strong community will push me to grow as a computer scientist, and I'm glad I have this opportunity to try new things at Leidos Dynetics!</p>
      </div>
    </div>

    <!-- Blog Post 2 -->
    <div class="blogPost" id = "post2">
      <div class="postContainer">
        <h1>Week 2 Recap: 6/2–6/6</h1>

        <p>I spent the majority of this week building foundational knowledge for my project. Like I said before, I don't have the strongest coding background coming into this internship, but I'm getting a lot of support to make sure I'm learning along the way. There were two main exercises I worked on: generating data that changes shape over time and training a classifier to identify circles when given data from persistence barcode plots. For the first of the two, I simulated points transforming from a circle to a square over a number of time steps and plotted the persistence barcode plots at each time step (see image below). For the latter, I wasn't able to completely finish the exercise this week, but I generated the data to train it on. The data consisted of 250 circles and non-circles (500 total), with varying amounts of noise. Some were mostly circular, some had randomized peaks sticking out of them, and some had a few points removed. Next week, I'll work on creating and training the classifier. Ideally by the end of these exercises, I'll be able to take my practice code and apply it to my project where I'll analyze the persistence of data that changes over time and train a classifier to identify patterns within that data.</p>

        <img src="./Images/plot3.gif" />

        <p>Outside of work, Parker, Daniel and I went on quite a few side quests. We were given tasks and special events by Mundy, and as video game completionists, we couldn't possibly turn down the adventure. Our first side quest: Leidos drone testing. We didn't do much ourselves, but we got to take (many) photos with the drones and talk with the people conducting the tests about their backgrounds and experiences at Leidos. A couple days later, we went on our second side quest to tour the Leidos facilities, ranging from the electronics lab to the manufacturing building. Our last quest (and in my opinion, the most daunting) was pickleball. Mundy is a pretty big pickleball fan, and everyone keeps saying Daniel is a great pickleball player, so we had to play pickleball at least once during the internship. Unfortunately, I have a history of acquiring a bloody nose every time I play a sport involving a ball, so I was fully prepared to end the day with a tissue up a nostril. Somehow, though, my nose remained dry the entire time, and my hand-eye coordination might have even improved! Jokes aside, I had a great time and a constant smile throughout the game, so I'm looking forward to future pickleball events.</p>
      </div>
    </div>

    <!-- Blog Post 3 -->
    <div class="blogPost" id = "post3">
      <div class="postContainer">
        <h1>Week 3 Recap: 6/9–6/13</h1>

        <p>My progress this week was a little slower than the week before, but in my defense, I think I tackled one of my largest knowledge barriers coming into this project: coding a classifier. For the first couple weeks, I was generating and plotting data while learning persistent homology. The former topic wasn't too far off from my previous experiences--I've used matplotlib and numpy for a few projects in my classes, and these small exercises of generating and plotting data were a great way to strengthen my understanding. The latter topic was newer, but persistent homology itself felt very intuitive, at least at the level that I needed to understand it at. Classifiers, on the other hand, felt foreign. I knew what they did but never had to create one and never learned the foundations of how they work. Fortunately, the internet is a great place, and I was able to create a basic circle/non-circle classifier with PyTorch through the help of many articles. My understanding of classifiers still wasn't at the level I wanted it to be by the end of the mini project, but Mundy's weekly AI/ML "bootcamp" helped me greatly. He covered the history and math of classifiers with Parker, Daniel, and me, and he eventually plans to have us create a classifier (and other AI projects) in Excel to improve our understanding of the math behind AI.</p>

        <p>While I knew I would be trying a lot of new things while working at Leidos, I wasn't expecting the learning to continue outside of the office--quite literally. Mundy invited the three of us to go on a foray to look for chanterelles, a type of edible mushroom, as well as other mushrooms. In the end, we didn't find many chanterelles, but we found and documented dozens of mushrooms on iNaturalist. Mundy also invited us to go to Puzzle Pint, a monthly puzzle event, after talking about our common interest in puzzles and hackathons. This activity was much more familiar to me, but it was a great experience nonetheless! We finished all the puzzles except the final metapuzzle, so we're planning to go back next month with the goal to finish everything.</p>
      </div>
    </div>

    <!-- Blog Post 4 -->
    <div class="blogPost" id = "post4">
      <div class="postContainer">
        <h1>Week 4 Recap: 6/16–6/20</h1>

        <p>The past three weeks have been jam packed with mini projects and activities, but this week was a little more relaxed because of Juneteenth. I spent the first few days familiarizing myself with Pandas and cleaning up the data I plan to use for my project. Unfortunately, the data isn't labeled with the examples I need to train the classifier, making this task a little more complicated. Instead of immediately working on the classifier, I've been working on clustering the persistence data into groups so an SME (subject matter expert) can label the data afterwards. I decided to use K-Means clustering as it's a rather common, general-purpose method, but I'm planning to spend some time next week trying other clustering methods and confirming that the groupings are correct. To visualize the 35-dimensional data points, I'm also going to experiment with dimensionality reduction methods like PCA (principal component analysis).</p>

        <p>The last couple of days of the week were spent working on creating the pipeline for the classifier: formatting the data, processing it (persistent homology), training the classifier, then testing its accuracy. Since most of the office was out for the holiday on Thursday, it was a rather chill day working on something I'd already experimented a bit with (see Week 3 Recap above). We eventually left the office a couple hours early to spend time with the other interns at Lowe Mill. It's arguably one of the coolest places I've been to so far in Huntsville, with displays featuring dozens of artists, and it was great getting to bond with other interns outside of the office.</p>
      </div>
    </div>

    <!-- Blog Post 5 -->
    <div class="blogPost" id = "post5">
      <div class="postContainer">
        <h1>Week 5 Recap: 6/23–6/27</h1>

        <p>Like I mentioned in last week's post, I wanted to spend time this week visualizing and verifying the cluster labels and finishing up the pipeline. I used PCA (principal component analysis) to reduce the dimensionality of the points from about 35 dimensions to just 3, resulting in the graph shown below. While the clustering itself doesn't look incorrect, verifying their accuracy is a little bit more difficult. After talking with Mundy, we determined that the best way to verify the accuracy of the clustering is to verify the accuracy of the pipeline as a whole (parsing data, clustering by persistence barcode, then training a classifier on those labels). In other words, if the entire method is accurate, including the clustering, the resulting classifier should be able to match hand-labeled data. As such, I plan to save and hand-label a portion of my data for validation. By the end of the week, I finished up the majority of the code and was able to test its functionality by inputting a portion of my data. I plan to spend next week cleaning up the program and finding more data to train on (further explained in the next blog post).</p>

        <img src="./Images/pca.png" />

        <p>While I made progress on my main project this week, I spent a good chunk of my time doing Mundy-specific intern responsibilities. I didn't mention it in previous posts, but Mundy has been hosting weekly Journal Club and AI Boot Camp assignments throughout the internship. This week, we were given three articles to read, all pertaining to information theory and its applications in different fields: "Organisms as a Special Kind of Information" by Mundy Reimer (our mentor), "Temperature as Joules per Bit" by Charles Alexandre Bédard et al., and "Simple Algorithmic Theory of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes" by Jürgen Schmidhuber. Though I'm likely biased, I enjoyed Mundy's paper the most as it related information theory to biology, a topic I hadn't really touched since going to college. His post covers several concepts in an intuitive manner all while explaining how information theory could be related to the interactions and growth of biological systems.</p>

        <p>For AI Boot Camp, our assignment for the week was to calculate the forward pass and backpropogation of a 3-layer MLP (multi-layer perceptron) by hand using Tom Yeh's "AI by Hand" Excel spreadsheet. When I first saw this series of words, I was entirely sure I would be watching 3Blue1Brown videos for hours just to even understand what the end goal was. After Mundy's lectures, though, I've learned that AI is just a lot of linear algebra and chain rule. Forward pass was a breeze--just a few MMULT (matrix product) and MAX cell functions in Excel. Calculating the backpropogation was a little more difficult. Filling in the spreadsheet with the proper functions was easy, but I wanted to understand the math behind the calculations in depth. By the end, I had calculated the partial derivative equations myself and even found a few typos in the workbook's equations.</p>
      </div>
    </div>

    <!-- Blog Post 6 -->
    <div class="blogPost" id = "post6">
      <div class="postContainer">
        <h1>Week 6 Recap: 6/30–7/4</h1>

        <p>To be completely honest, this week felt like a mental marathon. At the beginning of this internship, adjusting to the corporate lifestyle of working 8 to 5 with a 30-60 minute lunch was already an endurance test for my brain. College has spoiled me in that I only need to be in classes for at most 6 hours a day, if even that. I spend more time working on assignments later in the day, but I normally have a few hours in between to not think about classes while participating in clubs or working my part time job. I've adjusted to the 8 to 5 life by now, but July 4th threw our schedule for a loop. Parker, Daniel, and I wanted to drive back to Arkansas on Thursday to spend the holiday with our families, but we also wanted to get our full 40 hours in since interns are paid on an hourly basis (Note: for clarification, we were only expected to get 32 hours considering the holiday, but we decided to aim for 40). In the end, we condensed our work week and worked about 11 hours a day from Monday to Wednesday then left early on Thursday to make it back to Arkansas before midnight.</p>

        <p>Even though my work week was condensed, the work content was normal. Another week meant another Tom Yeh "AI by Hand" exercise from Mundy. This week was CNN (convolutional neural network) where we did a forward pass of a small CNN in Excel. This exercise felt a little more straightforward considering we only had to do a forward pass, and I was able to watch a few 3Blue1Brown videos to get a good conceptual understanding of how CNNs work. I also spent time finding data to train my classifier on. Like I said before, the data I currently have is extensive but lacks labels, and it's a rather big annoyance we've been trying to deal with. The general goal of the project is to look at persistence barcode plots of data over time and to train a classifier to label the data at each time step as anomalous or standard. This would be rather simple if the data included labels for what time frames are abnormal. Instead, I spent a good portion of my time finding these "anomalous events" in my data from news articles online and labeling the data from times before and after the anomaly as "standard events." While I could label a lot of data by hand, the goal of clustering by persistence barcodes was to automate that separation. The classifier will be trained on the anomalous events that I'm finding online with the labels given by clustering. Once I've collected enough data and finished training the classifier with the clustered labels, hopefully it will align with my hand-labeled validation data.</p>
      </div>
    </div>

    <!-- Blog Post 7 -->
    <div class="blogPost" id = "post7">
      <div class="postContainer">
        <h1>Week 7 Recap: 7/7–7/11</h1>

        <p>Mundy was out for a family event across the country, so we had limited communication and my progress on the classifier started to slow down. Thankfully, he brought up a secondary project idea that we talked about towards the beginning of the internship but put aside for later. The idea is loosely based around how transformers can look at sequential data, understand relationships between different parts of the data, then produce an output sequence. In relation to my project, I could input a series of persistent homology barcode vectors in chronological order then train a transformer to identify the last vector in the sequence. There were components of the idea that were familiar-persistent homology, vectorizing barcode plots, parsing data from JSON files-but it was also different enough that I had a lot to learn. I spent a few days reading up on the math behind transformers and building one using PyTorch as an exercise.</p>

        <p>While I still have much to learn about AI, I think my understanding of it has grown tenfold since coming here. The media tends to portray AI as a mysterious black box method that can solve the world's problems given enough time. In reality, a lot of common AI operations (e.g. applying weights, activating outputs, backpropogation) can be broken down into linear algebra, max functions, and a touch of chain rule. I enjoy being able to visualize MLPs, CNNs, and transformers as simple math equations instead of taking for granted that AI "just works."</p>

        <p>Outside of work, we had to face the biggest  challenge of our internship: the interns vs mentors kickball game. I'm quite terrible at kickball. Like I mentioned when talking about pickleball, I cannot figure out hand-eye coordination to save my life. My reflexes are even worse. I'm almost sure the people who could catch purposefully stayed near me when I was in the outfield to cover for me. Whether expected or not, us interns got absolutely destroyed. At some point, we just stopped counting because the point gap was so large. Despite the mentors' undeniable victory, I fully enjoyed playing kickball. It rained midway through, so we were all muddy and soaked by the end of the game, creating one of the greatest group photos of all time.</p>

        <img src="./Images/kickball1.jpg" class="kickball"/>
        <img src="./Images/kickball2.jpg" class="kickball"/>
      </div>
    </div>

    <!-- Blog Post 8 -->
    <div class="blogPost" id = "post8">
      <div class="postContainer">
        <h1>Week 8 Recap: 7/14–7/18</h1>

        <p>While I wasn't planning on writing about this, a certain other intern felt the need to reference me in <a href="https://daniel-nkunga.github.io/Blog/2025/07/18/Leidos-Dynetics-Week-8.html">their blog post...</a> Which brings us to the topic of analogue computing! At the beginning of the internship, Mundy asked the three of us to prepare presentations on topics and tools in computing as small side projects outside of our main work. The idea was for the three of us to rotate who presented each week to gain practice in public speaking, but time is a limited resource, and we've only had time to present <a href="https://daniel-nkunga.github.io/Blog/2025/06/20/Leidos-Dynetics-Week-4.html">Daniel's topic</a>. It's looking like Parker and I won't get the chance to present, so I might as well dedicate the majority of this blog post to ranting about my topic!</p>

        <p>For a little bit of background knowledge, let's go into the difference between digital and analogue computing. Digital computers are your stereotypical current-day computer--what you're probably reading this blog post on right now. They represent and manipulate information using discrete values (ex. 0's and 1's) and are able to do many tasks with high precision due to their flexibility in representing information. On the other hand, analogue computers tend to be highly specific. Data is represented by continuously variable physical states and quantities (ex. voltage, position, rotation, current), so if your analogue computer isn't designed for a task, you may need to make an entirely new computer. Despite these restrictions, analogue computers are useful for modeling differential systems with high precision (depending on the precision of the measuring device) and low energy usage.</p>

        <p>While I'd love to go into depth into all of the computers below, I'm resorting to listing them out and giving an extremely brief description of each.</p>

        <ul>- <a href="https://en.wikipedia.org/wiki/Antikythera_mechanism">Antikythera Mechanism</a>: Considered the first known mechanical analogue computer. Created in ~100 BCE by the ancient Greeks. Tracked the Sun and Moon, predicted astronomical events, indicated dates of athletic games like the Olympics.</ul>

        <ul>- <a href="https://www.youtube.com/watch?v=IgF3OX8nT0w">Tide-predicting Machines</a>: I couldn't possibly explain or visualize this better than Veritasium. First designed in 1872 by William Thompson. Does Fourier analysis of current tides then sums the sinusoidal functions to predict future tides.</ul>

        <ul>- <a href="https://www.analogmuseum.org/english/examples/vehicle_simulation/">Vehicle Suspension Systems</a>: Represents mechanical suspension systems (spring, mass, dashpot) as RLC circuits (resistor, inductor, capacitor) since it's expensive and difficult to fabricate, modify, and evaluate the systems in real life.</ul>

        <ul>- <a href="https://en.wikipedia.org/wiki/Billiard-ball_computer">Billiard Ball Computer</a>: Created in 1982 by Edward Fredkin and Tommaso Toffoli. A Thought experiment using collisions of balls to simulate logic gates. Later simulated by <a href="https://arxiv.org/abs/1204.1749">Yukio-Pegio Gunji</a> using swarms of <a href="https://en.wikipedia.org/wiki/Mictyris">soldier crabs</a> instead of <a href="https://www.technologyreview.com/2012/04/12/186779/computer-scientists-build-computer-using-swarms-of-crabs/">billiard balls</a>.</ul>

        <ul>- <a href="https://mythic.ai/technology/analog-computing/">Mythic AI Analogue Computing</a>: Developed by Mythic AI. Does analogue matrix/vector multiplication "in-memory" using tunable resistors. Avoids data movement bottlenecks since it "computes in memory" and is energy efficient, making it good for edge computing (ex. drones, cameras, and watches).</ul>

        <p>Last topic, I promise. This post is much longer than the previous ones, but I had to give respect to the marvel of technology that is analogue computing first before going into work topics. This week was a lot of rushing around to make a poster and PowerPoint presentation for the standard "end of internship" activities. I had done similar tasks in the past thanks to competing in ISEF in high school, but it was a slightly different experience making presentations for this internship. One of the biggest differences was the "accessibility" aspect as I like to call it. I have personal opinions on how science is communicated and think that people (including myself) can sometimes prioritize sounding intelligent by using technical language with little to no explanation over genuinely sharing knowledge and making science accessible. Yes, using technical language has its reasoning; for example, power and energy are similar words in casual use, but they are entirely different measurements in science, and that distinction is important. On the other hand, maybe we don't need to use a fancy word for plugging and unplugging something in all of our research papers (see <a href="https://www.lenovo.com/us/en/glossary/how-to-fix-noisy-computer/?orgRef=https%253A%252F%252Fwww.google.com%252F#:~:text=Reseat%2C%20in%20the%20context%20of%20technology%20and%20computing%2C%20refers%20to%20the%20process%20of%20removing%20and%20reinserting%20a%20component%20in%20its%20slot%20or%20connector.%20This%20is%20often%20done%20to%20ensure%20a%20good%20connection%20if%20the%20component%20was%20not%20properly%20seated%20initially%20or%20has%20become%20loose%20over%20time.">this article</a> on what reseat means) and maybe mathematicians need to chill a little with their notation (see image below by <a href="https://xkcd.com/2687/">xkcd</a>).</p>

        <img src="./Images/xkcd_division_notation.png"/>

        <p>While I'm sure it's not a trait unique to this internship, I enjoyed prioritizing the accessibility of my project while working on my presentations. It's a nice change of pace from research competitions filled with people trying to go far but instead landing in the pitfall of buzzwords and overcomplicated terminology. I know I've done the same in the past (and have kind of been doing the same with my blog for time and security reasons), but I put in a lot of effort with my presentations to make sure my audience understood all of the topics I brought up; I think it's been a great character arc.</p>

      </div>
    </div>

    <!-- Footer -->
    <div class="footer">
      <div class="footerContainer">
        <div class="footerText">
          <h2>Contact Information</h2>
          <p>Email: <a href="nasyachoy06@gmail.com">nasyachoy06@gmail.com</a></p>
          <p>LinkedIn: <a href="www.linkedin.com/in/nasyachoy">www.linkedin.com/in/nasyachoy</a></p>
          <p>Phone: (501) 472-5193</p>
        </div>
      </div>
    </div>
    <script src="app.js"></script>
  </body>
</html>
